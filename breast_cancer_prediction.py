# -*- coding: utf-8 -*-
"""breast_cancer_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x180YvRTr8QjPFqyN7nvsP4gLM2Paa0W
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve


# Set visualization styles for better-looking plots
plt.style.use('ggplot')  # Use ggplot style for a more professional look
sns.set_palette('Set2')  # Set a color palette for consistent colors
# %matplotlib inline  # Display plots directly in the notebook

# =====================================
# 1. DATA LOADING AND INITIAL EXPLORATION
# =====================================

# Load the breast cancer dataset from GitHub repository
url = "https://raw.githubusercontent.com/abdelDebug/Breast-Cancer-Dataset/main/breast_cancer_dataset.csv"
df = pd.read_csv(url)  # Read the CSV file into a pandas DataFrame

# Display basic information about the dataset
print("\nDataset shape:", df.shape)  # Shows number of rows and columns
print("\nFirst 5 rows:")  # Preview the first few rows to understand the data structure
display(df.head())

# Check data types and identify any missing values
print("\nDataset info:")
print(df.info())  # Shows column names, data types, and non-null values

print("\nMissing values per column:")
print(df.isnull().sum())  # Count missing values in each column

# =====================================
# 2. DATA EXPLORATION
# =====================================
print("2. DATA EXPLORATION")

# 2.1 Target variable distribution - Understanding class balance
print("\n2.1 Target Variable Distribution:")
target_counts = df['diagnosis'].value_counts()  # Count M (Malignant) and B (Benign) cases
print(target_counts)
# Calculate percentages to understand class balance
print(f"Percentage of Malignant: {100 * target_counts['M'] / len(df):.2f}%")
print(f"Percentage of Benign: {100 * target_counts['B'] / len(df):.2f}%")

# Create a pie chart to visualize the class distribution
plt.figure(figsize=(8, 6))
plt.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', startangle=90,
        colors=['#FF9999', '#66B2FF'])  # Red for Malignant, Blue for Benign
plt.title('Distribution of Diagnosis (Malignant vs Benign)')
plt.axis('equal')  # Equal aspect ratio ensures pie is drawn as a circle
plt.tight_layout()
plt.show()

# 2.2 Enhanced Descriptive Statistics - Examining mean, median, standard deviation and more
print("\n2.2 Comprehensive Descriptive Statistics:")

# Get basic statistics for all features
stats_df = df.describe().T  # Transpose for better readability

# Add median explicitly (though it's already in describe() as 50%)
stats_df = stats_df.rename(columns={'50%': 'median'})

# Create a more focused view of key statistics
key_stats = stats_df[['mean', 'median', 'std', 'min', 'max']]
print("\nKey statistics for each feature:")
display(key_stats)

# Compare mean and median to check for skewness
skew_analysis = pd.DataFrame({
    'mean': stats_df['mean'],
    'median': stats_df['median'],
    'difference': stats_df['mean'] - stats_df['median'],
    'skewness': (stats_df['mean'] - stats_df['median'])/stats_df['std']
})
print("\nSkewness analysis (comparing mean and median):")
display(skew_analysis)

# Analyze coefficient of variation (CV) to compare relative variability
cv_analysis = pd.DataFrame({
    'mean': stats_df['mean'],
    'std': stats_df['std'],
    'cv': stats_df['std'] / stats_df['mean'] * 100  # CV as percentage
})
cv_analysis = cv_analysis.sort_values(by='cv', ascending=False)  # Sort by CV
print("\nVariability analysis (coefficient of variation):")
display(cv_analysis)

# Visualize the distribution of means and medians by feature type
# Group features by their base name (e.g., radius_mean, radius_se, radius_worst)
feature_base_names = sorted(list(set([col.split('_')[0] for col in df.columns if '_' in col])))

plt.figure(figsize=(15, 10))
for i, base_name in enumerate(feature_base_names[:6]):  # First 6 base features
    # Get columns with this base name
    cols = [col for col in df.columns if col.startswith(f"{base_name}_")]

    # Extract values for each column
    values = [df[col] for col in cols]
    labels = [col.split('_')[1] for col in cols]  # Extract suffix (mean, se, worst)

    plt.subplot(2, 3, i+1)
    plt.boxplot(values, labels=labels)
    plt.title(f'Distribution of {base_name} measurements')
    plt.ylabel('Value')
    plt.grid(True, linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# Compare mean, median and standard deviation between malignant and benign groups
print("\nComparison of statistics between malignant and benign tumors:")

# Group by diagnosis and calculate statistics
grouped_stats = df.groupby('diagnosis').agg(['mean', 'median', 'std'])

# Select a few important features for comparison
important_features = ['radius_mean', 'texture_mean', 'perimeter_mean',
                      'area_mean', 'smoothness_mean', 'concavity_mean']

for feature in important_features:
    print(f"\nStatistics for {feature}:")
    display(grouped_stats[feature])

    # Visualize the comparison
    plt.figure(figsize=(10, 6))

    # Set x positions for bars
    x = np.arange(3)
    width = 0.35

    # Get values for benign and malignant
    b_values = grouped_stats.loc['B', feature].values
    m_values = grouped_stats.loc['M', feature].values

    # Create grouped bar chart
    plt.bar(x - width/2, b_values, width, label='Benign')
    plt.bar(x + width/2, m_values, width, label='Malignant')

    plt.xlabel('Statistic')
    plt.ylabel('Value')
    plt.title(f'Comparison of {feature} statistics')
    plt.xticks(x, ['Mean', 'Median', 'Std Dev'])
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

# 2.3 Visualizations to understand data distributions and identify outliers

# Histograms for selected features - Shows distribution and separability by diagnosis
print("\n2.3 Feature Distributions and Outlier Detection:")
features_to_plot = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'concavity_mean']
plt.figure(figsize=(15, 10))
for i, feature in enumerate(features_to_plot):
    plt.subplot(2, 3, i+1)  # Create a 2x3 grid of subplots
    # Create histogram with KDE (density curve) split by diagnosis
    sns.histplot(data=df, x=feature, hue='diagnosis', kde=True, bins=30)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

# Boxplots to identify outliers and compare feature distributions between diagnoses
plt.figure(figsize=(15, 10))
for i, feature in enumerate(features_to_plot):
    plt.subplot(2, 3, i+1)
    sns.boxplot(x='diagnosis', y=feature, data=df)  # Boxplot grouped by diagnosis
    plt.title(f'Boxplot of {feature} by Diagnosis')
plt.tight_layout()
plt.show()

# 2.4 Correlation analysis to understand relationships between features
print("\n2.4 Correlation Analysis:")

# Encode diagnosis as numeric (1=Malignant, 0=Benign) for correlation calculation
df_corr = df.copy()
df_corr['diagnosis'] = df_corr['diagnosis'].map({'M': 1, 'B': 0})

# Calculate Pearson correlation coefficients between all features
corr_matrix = df_corr.corr()
print("\nTop 10 features correlated with diagnosis:")
# Sort and display features most correlated with diagnosis
print(corr_matrix['diagnosis'].sort_values(ascending=False)[:11])

# Create a correlation heatmap to visualize all feature correlations
plt.figure(figsize=(16, 12))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.tight_layout()
plt.show()

# Create a focused heatmap showing only the top correlated features with diagnosis
top_corr_features = corr_matrix['diagnosis'].abs().sort_values(ascending=False)[1:11].index
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix.loc[['diagnosis'] + list(top_corr_features), ['diagnosis'] + list(top_corr_features)],
            annot=True, cmap='coolwarm', center=0, fmt='.2f')
plt.title('Correlation of Top 10 Features with Diagnosis')
plt.tight_layout()
plt.show()

# =====================================
# 3. DATA PREPARATION
# =====================================
print("\n3. DATA PREPARATION")

# 3.1 Separate features and target variable
X = df.drop(['diagnosis', 'id'], axis=1)  # Features: remove diagnosis (target) and id (not useful)
y = df['diagnosis'].map({'M': 1, 'B': 0})  # Target: encode Malignant as 1, Benign as 0

print("\nFeatures shape:", X.shape)  # Number of samples and features
print("Target shape:", y.shape)      # Number of target values

# 3.2 Split data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y  # stratify ensures class balance is maintained
)

print(f"\nTraining set: {X_train.shape[0]} samples")
print(f"Testing set: {X_test.shape[0]} samples")

# 3.3 Standardize features to have mean=0 and variance=1
# This is important for distance-based algorithms and convergence
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit to training data and transform it
X_test_scaled = scaler.transform(X_test)        # Transform test data using same parameters

# Visualize the effect of scaling on selected features
plt.figure(figsize=(15, 6))
plt.subplot(1, 2, 1)
plt.boxplot(X_train[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean']])
plt.title('Before Scaling')
plt.xticks([1, 2, 3, 4], ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean'], rotation=45)

plt.subplot(1, 2, 2)
scaled_df = pd.DataFrame(X_train_scaled[:, :4], columns=['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean'])
plt.boxplot(scaled_df)
plt.title('After Scaling')
plt.xticks([1, 2, 3, 4], ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean'], rotation=45)

plt.tight_layout()
plt.show()

# =====================================
# 4. MODEL TRAINING
# =====================================
print("\n4. MODEL TRAINING")

# 4.1 Initialize models with appropriate parameters
models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),  # Linear model with increased iterations
    'Support Vector Machine': SVC(random_state=42, probability=True),           # Enable probability estimates for ROC
    'Decision Tree': DecisionTreeClassifier(random_state=42),                   # Simple tree-based model
    'Random Forest': RandomForestClassifier(random_state=42),                   # Ensemble of trees
    'K-Nearest Neighbors': KNeighborsClassifier()                               # Distance-based classifier
}

# 4.2 Train each model and store results for evaluation
results = {}
for name, model in models.items():
    print(f"\nTraining {name}...")
    model.fit(X_train_scaled, y_train)  # Train the model on scaled training data

    # Make predictions on test data
    y_pred = model.predict(X_test_scaled)

    # Calculate prediction probabilities for ROC curve if supported by the model
    try:
        y_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1 (Malignant)
    except:
        y_prob = None  # Some models might not support probability predictions

    # Store model and prediction results for later evaluation
    results[name] = {
        'model': model,
        'predictions': y_pred,
        'probabilities': y_prob
    }

# =====================================
# 5. MODEL EVALUATION
# =====================================
print("\n5. MODEL EVALUATION")

# 5.1 Calculate performance metrics for all models
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']
results_df = pd.DataFrame(index=models.keys(), columns=metrics)

for name, result in results.items():
    y_pred = result['predictions']
    y_prob = result['probabilities']

    # Calculate various performance metrics
    results_df.loc[name, 'Accuracy'] = accuracy_score(y_test, y_pred)  # Overall accuracy
    results_df.loc[name, 'Precision'] = precision_score(y_test, y_pred)  # Precision (positive predictive value)
    results_df.loc[name, 'Recall'] = recall_score(y_test, y_pred)  # Recall/Sensitivity
    results_df.loc[name, 'F1 Score'] = f1_score(y_test, y_pred)  # Harmonic mean of precision and recall

    # AUC-ROC requires probability predictions
    if y_prob is not None:
        results_df.loc[name, 'AUC-ROC'] = roc_auc_score(y_test, y_prob)
    else:
        results_df.loc[name, 'AUC-ROC'] = np.nan

# Format metrics as decimal values with 4 decimal places
results_df = results_df.applymap(lambda x: f"{x:.4f}" if not pd.isna(x) else x)

print("\nModel Performance Comparison:")
display(results_df)  # Display the table of performance metrics

# 5.2 Visualize confusion matrices to understand classification errors
plt.figure(figsize=(15, 10))
for i, (name, result) in enumerate(results.items()):
    plt.subplot(2, 3, i+1)
    cm = confusion_matrix(y_test, result['predictions'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f'{name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# 5.3 Plot ROC curves to visualize the trade-off between sensitivity and specificity
plt.figure(figsize=(10, 8))
for name, result in results.items():
    if result['probabilities'] is not None:
        fpr, tpr, _ = roc_curve(y_test, result['probabilities'])
        auc = roc_auc_score(y_test, result['probabilities'])
        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')

# Add diagonal reference line (random classifier)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for All Models')
plt.legend()
plt.tight_layout()
plt.show()

# 5.4 Generate detailed classification reports
for name, result in results.items():
    print(f"\nClassification Report for {name}:")
    print(classification_report(y_test, result['predictions']))

# =====================================
# 6. HYPERPARAMETER TUNING
# =====================================
print("\n6. HYPERPARAMETER TUNING")

# Use the pre-determined best parameters from previous grid search
# This saves computation time while still showing the tuning concept
best_params = {
    'n_estimators': 50,     # Number of trees in the forest
    'max_depth': 10,        # Maximum depth of each tree
    'min_samples_split': 2, # Minimum samples required to split a node
    'min_samples_leaf': 1   # Minimum samples required at a leaf node
}

print("\nBest parameters found:")
print(best_params)

# Train a new Random Forest model with the optimized hyperparameters
best_rf = RandomForestClassifier(
    n_estimators=best_params['n_estimators'],
    max_depth=best_params['max_depth'],
    min_samples_split=best_params['min_samples_split'],
    min_samples_leaf=best_params['min_samples_leaf'],
    random_state=42
)

# Fit the optimized model and make predictions
best_rf.fit(X_train_scaled, y_train)
y_pred_tuned = best_rf.predict(X_test_scaled)
y_prob_tuned = best_rf.predict_proba(X_test_scaled)[:, 1]

# Compare performance before and after tuning
print("\nRandom Forest Performance:")
print("Before tuning:")
print(f"Accuracy: {accuracy_score(y_test, results['Random Forest']['predictions']):.4f}")
print(f"Precision: {precision_score(y_test, results['Random Forest']['predictions']):.4f}")
print(f"Recall: {recall_score(y_test, results['Random Forest']['predictions']):.4f}")
print(f"F1 Score: {f1_score(y_test, results['Random Forest']['predictions']):.4f}")
print(f"AUC-ROC: {roc_auc_score(y_test, results['Random Forest']['probabilities']):.4f}")

print("\nAfter tuning:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_tuned):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_tuned):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_tuned):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_tuned):.4f}")
print(f"AUC-ROC: {roc_auc_score(y_test, y_prob_tuned):.4f}")

# =====================================
# 7. FEATURE IMPORTANCE ANALYSIS
# =====================================
print("\n7. FEATURE IMPORTANCE ANALYSIS")

# Extract feature importance from the optimized Random Forest model
feature_importance = best_rf.feature_importances_
feature_names = X.columns

# Sort features by importance in descending order
sorted_idx = np.argsort(feature_importance)[::-1]
sorted_features = [feature_names[i] for i in sorted_idx]
sorted_importance = feature_importance[sorted_idx]

# Visualize the top 15 most important features
plt.figure(figsize=(12, 8))
plt.barh(range(len(sorted_features[:15])), sorted_importance[:15], align='center')
plt.yticks(range(len(sorted_features[:15])), sorted_features[:15])
plt.xlabel('Importance')
plt.title('Top 15 Feature Importance from Random Forest')
plt.tight_layout()
plt.show()

# Print the top 5 most important features with their importance values
print("\nTop 5 most important features:")
for i in range(5):
    print(f"{sorted_features[i]}: {sorted_importance[i]:.4f}")

# =====================================
# 8. CONCLUSION
# =====================================
print("\n8. CONCLUSION")

# Summarize the findings
print("\nSummary of findings:")
print(f"- Best performing model: Random Forest (tuned)")
print(f"- Accuracy: {accuracy_score(y_test, y_pred_tuned):.4f}")
print(f"- F1 Score: {f1_score(y_test, y_pred_tuned):.4f}")
print(f"- AUC-ROC: {roc_auc_score(y_test, y_prob_tuned):.4f}")

print("\nTop predictive features:")
for i in range(3):
    print(f"- {sorted_features[i]}: {sorted_importance[i]:.4f}")

print("\nKey statistical findings:")
print("- Malignant tumors show significantly larger mean values for radius, perimeter, and area")
print("- Features related to concavity and concave points show strong correlation with malignancy")
print("- Texture measurements show higher variability (coefficient of variation) compared to other features")